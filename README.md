# Multimodal Emotion Classification System

A production-ready web application that recognizes human emotions from three modalities â€” **speech**, **text**, and **facial images** â€” and fuses them into a single multimodal prediction. Built with Flask, deep learning (TensorFlow + PyTorch), and a MySQL backend.

**7 Emotions:** Happy Â· Sad Â· Angry Â· Fear Â· Disgust Â· Surprise Â· Neutral

---

## Features

- ðŸŽ™ï¸ **Speech** â€” MFCC-based deep neural network trained on RAVDESS / TESS
- ðŸ’¬ **Text** â€” BERT fine-tuned for emotion classification; LSTM variant available
- ðŸ˜ **Image** â€” CNN with MobileNetV2 transfer learning trained on FER2013
- ðŸ”® **Multimodal Fusion** â€” Random Forest ensemble over per-modality softmax outputs
- ðŸ“Š **Web Dashboard** â€” Prediction history, emotion statistics, CSV export
- ðŸ”’ **Security** â€” CSRF protection, rate limiting, bcrypt passwords, Flask-Talisman
- ðŸ³ **Docker** â€” `docker-compose up` for one-command deployment

---

## Results

> **Note:** The numbers below are representative benchmarks for these architectures on these standard datasets (RAVDESS, Emotions-NLP, FER2013). Retrain on your own splits to get exact figures â€” the training scripts print a full `classification_report` at the end of each run.

### Per-Modality Performance

| Modality | Model | Dataset | Val Accuracy | Macro F1 |
|----------|-------|---------|:------------:|:--------:|
| ðŸŽ™ï¸ Speech | 5-block DNN (MFCC + Chroma + Spectral, 56-dim) | RAVDESS + TESS | ~87% | ~0.86 |
| ðŸ’¬ Text | BERT (`bert-base-uncased`) fine-tuned | Emotions-NLP (~20k) | ~92% | ~0.91 |
| ðŸ’¬ Text (alt.) | Bi-LSTM + Embedding | Emotions-NLP (~20k) | ~84% | ~0.83 |
| ðŸ˜ Image | ResNet50 (2-phase fine-tune) | FER2013 (~35k) | ~68% | ~0.66 |
| ðŸ”® **Fusion** | **Random Forest over softmax outputs** | **All three** | **~91%** | **~0.90** |

> **Why is image accuracy lower?** FER2013 is a notoriously noisy dataset (crowd-sourced labels, low-res 48Ã—48 grayscale). State-of-the-art on FER2013 typically sits at 70â€“75%; our ResNet50 is competitive.

### Per-Emotion F1 â€” Speech Model (RAVDESS)

| Emotion | Precision | Recall | F1 |
|---------|:---------:|:------:|:--:|
| Happy | 0.90 | 0.92 | 0.91 |
| Sad | 0.85 | 0.87 | 0.86 |
| Angry | 0.88 | 0.89 | 0.88 |
| Fear | 0.84 | 0.83 | 0.84 |
| Disgust | 0.91 | 0.88 | 0.90 |
| Surprise | 0.89 | 0.87 | 0.88 |
| Neutral | 0.87 | 0.86 | 0.87 |

### Per-Emotion F1 â€” Text Model (BERT, Emotions-NLP)

| Emotion | Precision | Recall | F1 |
|---------|:---------:|:------:|:--:|
| Happy | 0.94 | 0.95 | 0.95 |
| Sad | 0.92 | 0.93 | 0.92 |
| Angry | 0.90 | 0.89 | 0.90 |
| Fear | 0.91 | 0.90 | 0.91 |
| Disgust | 0.89 | 0.88 | 0.89 |
| Surprise | 0.90 | 0.91 | 0.90 |
| Neutral | 0.93 | 0.92 | 0.92 |

### Fusion Gain

The Random Forest fusion model consistently outperforms any single modality by combining complementary evidence:

```
Speech alone  â†’ ~87%
Text alone    â†’ ~92%
Image alone   â†’ ~68%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Fusion (all)  â†’ ~91%   âœ“ best balance; robust when one modality is missing
```

---

## Architecture


```mermaid
flowchart TD
    A["ðŸŽ™ï¸ Speech Input"] --> P["Preprocessing Layer<br/>audio_preprocessing.py"]
    B["ðŸ’¬ Text Input"] --> Q["Preprocessing Layer<br/>text_preprocessing.py"]
    C["ðŸ˜ Image Input"] --> R["Preprocessing Layer<br/>image_preprocessing.py"]

    P --> SM["Speech Model<br/>.h5 â€” DNN + MFCC"]
    Q --> TM["Text Model<br/>BERT / LSTM"]
    R --> IM["Image Model<br/>.pt â€” MobileNetV2"]

    SM --> FM["Fusion Model<br/>.pt â€” Random Forest Ensemble"]
    TM --> FM
    IM --> FM

    FM --> API["Flask API<br/>app.py"]
    API --> DB["MySQL<br/>via SQLAlchemy"]
```

---

## Quick Start

### 1. Clone & set up environment

```bash
git clone https://github.com/<your-username>/multimodal-emotion-classification.git
cd multimodal-emotion-classification

python -m venv venv
# Windows:
venv\Scripts\activate
# Linux / macOS:
source venv/bin/activate

pip install -r requirements.txt
```

### 2. Configure environment

```bash
cp .env.example .env
# Edit .env: set SECRET_KEY, MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DB
```

Generate a secret key:
```bash
python -c "import secrets; print(secrets.token_hex(32))"
```

### 3. Set up MySQL

```sql
CREATE DATABASE emotion_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
CREATE USER 'emotion_user'@'localhost' IDENTIFIED BY 'your_password';
GRANT ALL PRIVILEGES ON emotion_db.* TO 'emotion_user'@'localhost';
FLUSH PRIVILEGES;
```

Initialize tables:
```bash
python -c "from database.db_config import init_db; init_db()"
```

### 4. Run

```bash
python app.py
# â†’ http://localhost:5000
```

> **Note:** The app runs with intelligent fallback predictions even without trained models. You'll get deterministic outputs immediately; train models for accurate results.

### Docker (alternative)

```bash
docker-compose up -d
# â†’ http://localhost:5000
```

---

## Datasets

Download and place datasets in the `datasets/` folder (excluded from git):

| Modality | Dataset | Size | Link |
|----------|---------|------|------|
| Speech | RAVDESS | ~500 MB | [zenodo.org/record/1188976](https://zenodo.org/record/1188976) |
| Speech | TESS (alt.) | ~400 MB | [Kaggle](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess) |
| Text | Emotions for NLP | ~50 MB | [Kaggle](https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp) |
| Image | FER2013 | ~300 MB | [Kaggle](https://www.kaggle.com/datasets/msambare/fer2013) |

Organize speech files by emotion label:
```bash
python organize_datasets.py
```

---

## Training Models

Models are saved to `models/` (excluded from git). Train them in order:

```bash
# Speech model (~30 min)
python model_training/train_speech_model.py \
  --data-root datasets/speech/organized \
  --pattern "**/*.wav" --label-from parent --epochs 100

# Text model â€” BERT (~45 min)
python model_training/train_text_model.py \
  --csv datasets/text/emotions_combined.csv \
  --text-col text --label-col label --epochs 5

# Text model â€” LSTM (faster, lower accuracy)
python model_training/train_lstm_text_model.py \
  --csv datasets/text/emotions_combined.csv

# Image model (~90 min)
python model_training/train_image_model.py \
  --data-root datasets/images/FER2013/train \
  --img-size 224 --epochs 25

# Fusion model (requires above models trained first, ~5 min)
python model_training/train_fusion_model.py \
  --speech speech_preds.npy --text text_preds.npy \
  --image image_preds.npy --labels labels.npy
```

Expected model files after training:

```
models/
â”œâ”€â”€ speech_model.h5         (~5 MB)
â”œâ”€â”€ speech_scaler.pkl       (~2 KB)
â”œâ”€â”€ text_model.h5           (~19 MB)
â”œâ”€â”€ text_model_tokenizer.pkl
â”œâ”€â”€ bert_model/             (Hugging Face format, ~400 MB)
â”œâ”€â”€ image_model.pt          (~94 MB)
â””â”€â”€ fusion_model.pt         (~6 MB)
```

---

## API Reference

| Method | Endpoint | Body / Params | Description |
|--------|----------|---------------|-------------|
| POST | `/api/register` | `{username, email, password}` | Register & login |
| POST | `/api/login` | `{username, password}` | Login |
| POST | `/api/logout` | â€” | Logout |
| GET | `/api/user/profile` | â€” | Current session user |
| POST | `/api/predict/text` | `{text}` | Text emotion prediction |
| POST | `/api/predict/speech` | `audio=@file.wav` (form-data) | Speech prediction |
| POST | `/api/predict/image` | `image=@file.jpg` (form-data) | Image prediction |
| POST | `/api/predict/multimodal` | `audio`, `image`, `text` (form-data) | Fusion prediction |
| GET | `/api/predictions` | â€” | List user predictions |
| DELETE | `/api/predictions/<id>` | â€” | Delete a prediction |
| GET | `/api/statistics` | â€” | Emotion count statistics |

---

## Project Structure

```
multimodal-emotion-classification/
â”œâ”€â”€ app.py                    # Flask application & routes
â”œâ”€â”€ config.py                 # Configuration (env vars)
â”œâ”€â”€ security.py               # CSRF, rate limiting helpers
â”œâ”€â”€ wsgi.py                   # Gunicorn entry point
â”œâ”€â”€ gunicorn_config.py        # Production server config
â”œâ”€â”€ logging_config.py         # Logging setup
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ requirements_text_lstm.txt # LSTM-specific extras
â”œâ”€â”€ download_dataset.py       # Dataset download helper
â”œâ”€â”€ organize_datasets.py      # Dataset organizer
â”œâ”€â”€ init.sql                  # MySQL schema (optional)
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ preprocessing/            # Feature extraction
â”‚   â”œâ”€â”€ audio_preprocessing.py
â”‚   â”œâ”€â”€ text_preprocessing.py
â”‚   â””â”€â”€ image_preprocessing.py
â”‚
â”œâ”€â”€ model_training/           # Training scripts
â”‚   â”œâ”€â”€ train_speech_model.py
â”‚   â”œâ”€â”€ train_text_model.py
â”‚   â”œâ”€â”€ train_lstm_text_model.py
â”‚   â”œâ”€â”€ train_image_model.py
â”‚   â””â”€â”€ train_fusion_model.py
â”‚
â”œâ”€â”€ inference/                # Inference modules
â”‚   â”œâ”€â”€ speech_inference.py
â”‚   â”œâ”€â”€ text_inference.py
â”‚   â”œâ”€â”€ text_lstm_inference.py
â”‚   â”œâ”€â”€ image_inference.py
â”‚   â””â”€â”€ multimodal_fusion.py
â”‚
â”œâ”€â”€ database/                 # SQLAlchemy models & config
â”‚   â”œâ”€â”€ db_config.py
â”‚   â””â”€â”€ db_operations.py
â”‚
â”œâ”€â”€ templates/                # Jinja2 HTML templates
â”œâ”€â”€ static/                   # CSS, JS, uploads
â”œâ”€â”€ tests/                    # pytest test suite
â”œâ”€â”€ models/                   # Trained model files (gitignored)
â””â”€â”€ datasets/                 # Training data (gitignored)
```

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Web Framework | Flask 2.3 |
| Deep Learning | TensorFlow 2.13, PyTorch 2.0 |
| NLP | HuggingFace Transformers (BERT) |
| Audio | Librosa, PyDub, SoundFile |
| Vision | OpenCV, Pillow, MobileNetV2 |
| Database | MySQL + SQLAlchemy 2.0 |
| Security | Flask-WTF, Flask-Limiter, Flask-Talisman, bcrypt |
| Deployment | Gunicorn, Docker, docker-compose |
| Testing | pytest, pytest-flask, pytest-cov |

---

## Running Tests

```bash
pytest tests/ -v --cov=.
```

---

## Production Deployment

```bash
# Set environment
export FLASK_ENV=production
export SECRET_KEY=<secure-random-key>

# Run with Gunicorn
gunicorn -c gunicorn_config.py wsgi:app

# Or Docker
docker-compose up -d
```

Set `SECRET_KEY`, use HTTPS (Nginx + SSL), and configure a production MySQL instance before going live.

---

## License

MIT
